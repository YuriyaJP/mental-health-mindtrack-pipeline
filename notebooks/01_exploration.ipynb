{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e516aa8",
   "metadata": {},
   "source": [
    "# Dataset Exploration: Suicide_Detection.csv\n",
    "\n",
    "This notebook performs a full exploratory data analysis (EDA) on the raw dataset located at `data/raw/Suicide_Detection.csv`.\n",
    "\n",
    "Sections:\n",
    "- Imports and load data\n",
    "- Basic overview (shape, columns, dtypes, head)\n",
    "- Missing values and summary statistics\n",
    "- Target detection and class balance\n",
    "- Text column detection, text-length distribution and top words\n",
    "- Visualizations and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38acd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports: standard EDA libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# plotting defaults\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c889db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate and load the CSV file\n",
    "data_path = Path('..') / 'data' / 'raw' / 'Suicide_Detection.csv'\n",
    "# if running from repo root in Notebook, adjust path: try both local and repo-root aware\n",
    "if not data_path.exists():\n",
    "    data_path = Path('data') / 'raw' / 'Suicide_Detection.csv'\n",
    "print('Using data file:', data_path)\n",
    "\n",
    "# read with pandas (let pandas infer encoding/types). If file is large, consider nrows preview first.\n",
    "df = pd.read_csv(data_path)\n",
    "print('Loaded, shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c89eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic overview\n",
    "display(df.head(10))\n",
    "print('\\nColumns and dtypes:')\n",
    "display(pd.DataFrame({'column': df.columns, 'dtype': df.dtypes.astype(str)}))\n",
    "\n",
    "print('\\nDataFrame info:')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "display(df.describe(include='number').T)\n",
    "\n",
    "# Summary for object / categorical columns (top / freq)\n",
    "display(df.describe(include='object').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values: counts and percentage\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_table = pd.DataFrame({'missing_count': missing, 'missing_percent': missing_pct})\n",
    "display(missing_table[missing_table['missing_count']>0])\n",
    "\n",
    "# Visualize missingness if there are any missing values\n",
    "if missing_table['missing_count'].sum() > 0:\n",
    "    plt.figure(figsize=(10, max(2, 0.3 * missing_table.shape[0])))\n",
    "    sns.heatmap(df.isna(), cbar=False)\n",
    "    plt.title('Missing values heatmap (rows x columns)')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Rows')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No missing values detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c65a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to detect a target / label column automatically\n",
    "# Heuristics: columns named 'label','target','class','suicide' or columns with small unique values (<=10) and type int/object.\n",
    "possible_names = ['label','target','class','suicide','suicidal']\n",
    "target_col = None\n",
    "for name in possible_names:\n",
    "    if name in df.columns:\n",
    "        target_col = name\n",
    "        break\n",
    "if target_col is None:\n",
    "    # look for low-cardinality columns (but not an id column)\n",
    "    for c in df.columns:\n",
    "        if (df[c].nunique(dropna=False) <= 10) and (df[c].nunique() > 1):\n",
    "            # avoid columns that look like ids (unique for each row)\n",
    "            if df[c].nunique() / len(df) < 0.95:\n",
    "                target_col = c\n",
    "                break\n",
    "\n",
    "print('Detected target column:', target_col)\n",
    "if target_col is not None:\n",
    "    vc = df[target_col].value_counts(dropna=False)\n",
    "    display(vc.to_frame(name='count'))\n",
    "    display((vc / len(df) * 100).round(2).to_frame(name='percent'))\n",
    "    # plot class distribution\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(y=target_col, data=df, order=vc.index)\n",
    "    plt.title('Class distribution for ' + str(target_col))\n",
    "    plt.xlabel('count')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No clear target column detected automatically. Consider specifying it manually.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect likely text columns (object dtype with relatively long contents)\n",
    "text_candidates = []\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    median_len = df[c].dropna().astype(str).map(len).median() if df[c].notna().any() else 0\n",
    "    mean_len = df[c].dropna().astype(str).map(len).mean() if df[c].notna().any() else 0\n",
    "    if mean_len > 20 or median_len > 10:\n",
    "        text_candidates.append((c, int(mean_len), int(median_len)))\n",
    "\n",
    "print('Text column candidates (column, mean_len, median_len):')\n",
    "display(pd.DataFrame(text_candidates, columns=['column','mean_len','median_len']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54956d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we found a text column, show length distribution and top words\n",
    "if text_candidates:\n",
    "    text_col = text_candidates[0][0]\n",
    "    print('Using text column:', text_col)\n",
    "    # text length distribution\n",
    "    lengths = df[text_col].dropna().astype(str).map(len)\n",
    "    display(lengths.describe().to_frame(name='text_length_stats'))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(lengths, bins=50, kde=True)\n",
    "    plt.title('Text length distribution for ' + text_col)\n",
    "    plt.xlabel('length (chars)')\n",
    "    plt.show()\n",
    "\n",
    "    # Top words overall (simple)\n",
    "    vect = CountVectorizer(stop_words='english', max_features=2000)\n",
    "    docs = df[text_col].fillna('').astype(str)\n",
    "    X = vect.fit_transform(docs)\n",
    "    word_counts = np.array(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vect.get_feature_names_out())\n",
    "    top_idx = np.argsort(word_counts)[-30:][::-1]\n",
    "    top_words = pd.DataFrame({'word': vocab[top_idx], 'count': word_counts[top_idx]})\n",
    "    display(top_words)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(data=top_words, y='word', x='count')\n",
    "    plt.title('Top 30 words (stopwords removed)')\n",
    "    plt.show()\n",
    "\n",
    "    # Top words by class if target found\n",
    "    if target_col is not None:\n",
    "        for cls in df[target_col].dropna().unique():\n",
    "            mask = df[target_col] == cls\n",
    "            docs_cls = df.loc[mask, text_col].fillna('').astype(str)\n",
    "            Xc = vect.transform(docs_cls)\n",
    "            wc = np.array(Xc.sum(axis=0)).ravel()\n",
    "            top_idx_c = np.argsort(wc)[-15:][::-1]\n",
    "            print(f'\\nTop words for class {cls}:')\n",
    "            display(pd.DataFrame({'word': vocab[top_idx_c], 'count': wc[top_idx_c]}))\n",
    "else:\n",
    "    print('No text-like column detected automatically. If your text column is named differently, set text_col manually and re-run analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd331d",
   "metadata": {},
   "source": [
    "## Quick conclusions and suggested next steps\n",
    "\n",
    "- Review the detected `target_col` above â€” if it's incorrect, set `target_col = '<your_column>'` and re-run the class-balance and top-words cells.\n",
    "- If there are missing values in important columns, decide on imputation or row removal strategies.\n",
    "- For text processing: clean punctuation, lowercase, consider lemmatization, and try TF-IDF or embeddings for modelling.\n",
    "- If classes are imbalanced: consider stratified splits, resampling, or class-weighted models.\n",
    "- Save a cleaned subset for model development under `data/processed/` after you finalize preprocessing choices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
